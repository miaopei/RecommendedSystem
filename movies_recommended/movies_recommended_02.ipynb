{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 电影推荐实例--基于协同过滤和DL特征提取的比较\n",
    "\n",
    "> [电影推荐实例--基于协同过滤和DL特征提取的比较](https://blog.csdn.net/qq_32453673/article/details/72593675)\n",
    "\n",
    "## 1. 数据集\n",
    "\n",
    "来源于 [MovieLens](https://grouplens.org/datasets/movielens/) 中的 ml-latest-small.zip，当然也可以从本文最后我的[github](https://github.com/elliotzhao/Recommend-movies) 中找到。 \n",
    "\n",
    "本项目主要用到其中的两个csv文件：\n",
    "\n",
    "- ml-latest-small/ratings.csvz：包含 671 个用户，共 100004 个打分\n",
    "\n",
    "- ml-latest-small/links.csv：包含 9125 个电影及其 IMDBid 和 TMDBid\n",
    "\n",
    "分别长这样：\n",
    "\n",
    "```shell\n",
    "#ratings.csvz：\n",
    "   userId  movieId  rating   timestamp\n",
    "0       1       31     2.5  1260759144\n",
    "1       1     1029     3.0  1260759179\n",
    "2       1     1061     3.0  1260759182\n",
    "3       1     1129     2.0  1260759185\n",
    "4       1     1172     4.0  1260759205\n",
    "......\n",
    "\n",
    "#links.csv：\n",
    "   movieId  imdbId   tmdbId\n",
    "0        1  114709    862.0\n",
    "1        2  113497   8844.0\n",
    "2        3  113228  15602.0\n",
    "3        4  114885  31357.0\n",
    "4        5  113041  11862.0\n",
    "......\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 理论背景\n",
    "\n",
    "粗略地说，推荐系统有三种类型（不包括简单的评级方法）：\n",
    "\n",
    "- 基于内容的推荐\n",
    "\n",
    "- 协同过滤\n",
    "\n",
    "- 混合模型\n",
    "\n",
    "“基于内容的推荐”是一个回归问题，我们把电影内容作为特征，对用户对电影的评分做预测。\n",
    "\n",
    "“协同过滤”中，一般无法提前获得内容特征。是通过用户之间的相似度（用户们给了用一个电影相同的评级）和电影之间的相似度（有相似用户评级的电影）来学习潜在特征，同时预测用户对电影的评分。在学习了电影的特征之后，我们便可以衡量电影之间的相似度，并根据用户历史观影信息，向他/她推荐最相似的电影。\n",
    "\n",
    "“基于内容的推荐”和“协同过滤”是10多年前最先进的技术。很显然，现在有很多模型和算法可以提高预测效果。比如，针对事先缺乏用户电影评分信息的情况，可以使用隐式矩阵分解，用偏好和置信度取代用户电影打分——比如用户对电影推荐有多少次点击，以此进行协同过滤。另外，我们还可以将“内容推荐”与“协同过滤”的方法结合起来，将内容作为侧面信息来提高预测精度。这种混合方法，可以用“学习进行排序”（”Learning to Rank” ）算法来实现。\n",
    "\n",
    "在该项目中，采用的方法是“协同过滤”。首先，用电影和用户相似度来找出相似度最高的海报，并基于相似度做电影推荐。然后，我将讨论如何Deep Learning学习潜在特征、做电影推荐。最后会谈谈如何在推荐系统中使用深度学习。\n",
    "\n",
    "## 3. 电影相似性\n",
    "\n",
    "对于基于协同过滤的推荐系统，首先要建立评分矩阵。其中，每一行表示一个用户，每一列对应其对某一电影的打分。建立的评分矩阵如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rating_f = 'ml-latest-small/ratings.csv'\n",
    "link_f   = 'ml-latest-small/links.csv'\n",
    "\n",
    "df    = pd.read_csv(rating_f, sep=',')\n",
    "df_id = pd.read_csv(link_f, sep=',')\n",
    "df    = pd.merge(df, df_id, on=['movieId'])\n",
    "\n",
    "rating_matrix = np.zeros((df.userId.unique().shape[0], max(df.movieId)))\n",
    "for row in df.itertuples():\n",
    "    rating_matrix[row[1] - 1, row[2] - 1] = row[3]\n",
    "rating_matrix = rating_matrix[:,:9000] #get first 9000 movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“ratings.csv”包含用户id，电影id, 评级，和时间信息，其中同一个用户id可能对多个电影进行打分；\n",
    "\n",
    "“links.csv”包含电影id, IMDB id,和TMDB id。\n",
    "\n",
    "这两个文件中包含的电影总数是不同的（rating.csv: 9066, links.csv: 9125）。我们将两个文件依movieId合并，并在此基础上得到评分矩阵rating_matrix[userId:movieId]=grade。\n",
    "\n",
    "计算评分矩阵的稀疏性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity is 1.4030468620632555%\n"
     ]
    }
   ],
   "source": [
    "#Evaluate sparsity of matrix\n",
    "sparsity = float(len(rating_matrix.nonzero()[0]))\n",
    "sparsity /= (rating_matrix.shape[0] * rating_matrix.shape[1])\n",
    "sparsity *= 100\n",
    "print('Sparsity is {0}%'.format(sparsity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以得到sparsity=1.4%，认为是稀疏矩阵。 \n",
    "\n",
    "现在，为了训练和测试，我们将评分矩阵分解成两个矩阵，从每行（userId）评分矩阵中抠出了10个评分，将其放入测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Splite to train/test matrix\n",
    "train_matrix = rating_matrix.copy()\n",
    "test_matrix  = np.zeros(rating_matrix.shape)\n",
    "\n",
    "for i in range(rating_matrix.shape[0]):\n",
    "        rating_index = np.random.choice(rating_matrix[i].nonzero()[0], size=10, replace=True) #return a list\n",
    "        train_matrix[i, rating_index] = 0.0\n",
    "        test_matrix[i, rating_index] = rating_matrix[i, rating_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据余弦相似性计算两个特征间的夹角（具体过程参见另外一篇博文：[余弦相似定理和新闻分类](https://blog.csdn.net/qq_32453673/article/details/72615773)）： \n",
    "\n",
    "$$cos(\\theta )=\\frac{<b,c>}{\\mid b\\mid\\cdot \\mid c\\mid}$$\n",
    "\n",
    "即：\n",
    "\n",
    "$$cos(\\theta )=\\frac{x_{1}y_{1}+x_{2}y_{2}+\\cdot \\cdot \\cdot +x_{n}y_{n} }{\\sqrt{x_{1}^{2}+x_{2}^{2}+\\cdot \\cdot \\cdot +x_{n}^{2}}\\cdot \\sqrt{y_{1}^{2}+y_{2}^{2}+\\cdot \\cdot \\cdot +y_{n}^{2}}}$$\n",
    "\n",
    "同理计算用户/电影中的余弦相似性： \n",
    "\n",
    "$$s(u,v)=\\frac{r_{u}r_{v}}{\\parallel r_{u}\\parallel \\parallel r_{v}\\parallel }$$\n",
    "\n",
    "这里 s(u,v) 是用户 u 和 v 之间的余弦相似度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cosine similarity\n",
    "similarity_usr = train_matrix.dot(train_matrix.T) + 1e-9\n",
    "norms = np.array([np.sqrt(np.diagonal(similarity_usr))])\n",
    "similarity_usr = (similarity_usr / (norms * norms.T))\n",
    "\n",
    "similarity_mv = train_matrix.T.dot(train_matrix) + 1e-9\n",
    "norms = np.array([np.sqrt(np.diagonal(similarity_mv))])\n",
    "similarity_mv = (similarity_mv / (norms * norms.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们利用其他所有用户对某一电影i的评分来预测当前用户u对该电影的评分，并且用相似度作为权重，然后标准化： \n",
    "\n",
    "$$\\hat{r}_{uv}=\\frac{\\sum_{v} s(u,v)r_{vi}}{\\sum_{v}\\mid s(u,v)\\mid }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 9.864265738978938\n"
     ]
    }
   ],
   "source": [
    "prediction = similarity_usr.dot(train_matrix) / np.array([np.abs(similarity_usr).sum(axis=1)]).T\n",
    "prediction = prediction[test_matrix.nonzero()]\n",
    "test_vector = test_matrix[test_matrix.nonzero()]\n",
    "mse = mean_squared_error(prediction, test_vector)\n",
    "\n",
    "print('mse: {0}'.format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测的MSE是9.8。到此我们实现了根据其他用户的评分来预测某一个用户对某个电影的评分并且进行评价。\n",
    "\n",
    "但这个数根本看不出来是什么鬼，也不知道预测的怎么样。因此我们换一个直观的方法，直接看相似度：由于矩阵similarity_mv[i,j]表示第i个电影和第j个电影的相关性，所以对于第i个电影找出该行最大的5个值所在的列号，就是和这个电影最相似的电影的movieId。 \n",
    "我们使用IMDB id，使用API从Movie Database 网站获取海报。\n",
    "\n",
    "从“links.csv”获得movieId和IMDB id的映射关系："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_display   = 5\n",
    "base_mv_idx = 0\n",
    "idx_to_mv   = {}\n",
    "\n",
    "for row in df_id.itertuples():\n",
    "        idx_to_mv[row[1]-1] = row[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base_mv_idx表示要找出和这个movieId相似的电影，n_display表示找这么多个相似的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mv = [idx_to_mv[x] for x in np.argsort(similarity_mv[base_mv_idx])[:-n_display-1:-1]]\n",
    "mv = filter(lambda imdb: len(str(imdb))==6, mv)\n",
    "mv = list(mv)[:n_display]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面要把这n_display个电影的海报通过API从Movie Database上扒下来，先获得存储它们的URL："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "#Get posters from Movie Database by API\n",
    "headers = {'Accept':'application/json'}\n",
    "payload = {'api_key':'20047cd838219fb54d1f8fc32c45cda4'}\n",
    "response = requests.get('http://api.themoviedb.org/3/configuration',params=payload,headers=headers)\n",
    "response = json.loads(response.text)\n",
    "\n",
    "base_url = response['images']['base_url']+'w185'\n",
    "\n",
    "def get_poster(imdb,base_url):\n",
    "    #query themovie.org API for movie poster path.\n",
    "    imdb_id = 'tt0{0}'.format(imdb)\n",
    "    movie_url = 'http://api.themoviedb.org/3/movie/{:}/images'.format(imdb_id)\n",
    "    response = requests.get(movie_url,params=payload,headers=headers)\n",
    "    try:\n",
    "        file_path = json.loads(response.text)['posters'][0]['file_path']\n",
    "    except:\n",
    "        print('Something wrong, cannot get the poster for imdb id: {0}!'.format(imdb))\n",
    "\n",
    "    return base_url+file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "URL = [0] * len(list(mv))\n",
    "\n",
    "for i, m in enumerate(mv):\n",
    "    URL[i] = get_poster(m, base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据得到的URL将这n_display个电影的海报一起显示出来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://image.tmdb.org/t/p/w185/uMZqKhT4YA6mqo2yczoznv7IDmv.jpg\n",
      "http://image.tmdb.org/t/p/w185/kuTPkbQmHxBHsxaKMUL1kUchhdE.jpg\n",
      "http://image.tmdb.org/t/p/w185/z4ROnCrL77ZMzT0MsNXY5j25wS2.jpg\n",
      "http://image.tmdb.org/t/p/w185/bqLlWZJdhrS0knfEJRkquW7L8z2.jpg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img style='width: 100px; margin: 0px; float: left; border: 1px solid black;' src='http://image.tmdb.org/t/p/w185/uMZqKhT4YA6mqo2yczoznv7IDmv.jpg' /><img style='width: 100px; margin: 0px; float: left; border: 1px solid black;' src='http://image.tmdb.org/t/p/w185/kuTPkbQmHxBHsxaKMUL1kUchhdE.jpg' /><img style='width: 100px; margin: 0px; float: left; border: 1px solid black;' src='http://image.tmdb.org/t/p/w185/z4ROnCrL77ZMzT0MsNXY5j25wS2.jpg' /><img style='width: 100px; margin: 0px; float: left; border: 1px solid black;' src='http://image.tmdb.org/t/p/w185/bqLlWZJdhrS0knfEJRkquW7L8z2.jpg' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "\n",
    "images = ''\n",
    "for i in range(len(mv)):\n",
    "    print(URL[i])\n",
    "    images+=\"<img style='width: 100px; margin: 0px; float: left; border: 1px solid black;' src='%s' />\" % URL[i]\n",
    "\n",
    "    #print(images)\n",
    "display(HTML(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity is 1.4030468620632555%\n",
      "mse: 9.657992950879118\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img style='width: 100px; margin: 0px; float: left; border: 1px solid black;' src='http://image.tmdb.org/t/p/w185/uMZqKhT4YA6mqo2yczoznv7IDmv.jpg' /><img style='width: 100px; margin: 0px; float: left; border: 1px solid black;' src='http://image.tmdb.org/t/p/w185/kuTPkbQmHxBHsxaKMUL1kUchhdE.jpg' /><img style='width: 100px; margin: 0px; float: left; border: 1px solid black;' src='http://image.tmdb.org/t/p/w185/bqLlWZJdhrS0knfEJRkquW7L8z2.jpg' /><img style='width: 100px; margin: 0px; float: left; border: 1px solid black;' src='http://image.tmdb.org/t/p/w185/z4ROnCrL77ZMzT0MsNXY5j25wS2.jpg' /><img style='width: 100px; margin: 0px; float: left; border: 1px solid black;' src='http://image.tmdb.org/t/p/w185/93Y9BGx8blzmZOPSoivkFfaifqU.jpg' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 完整代码\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rating_f = 'ml-latest-small/ratings.csv'\n",
    "link_f   = 'ml-latest-small/links.csv'\n",
    "\n",
    "df = pd.read_csv(rating_f,sep=',')\n",
    "df_id = pd.read_csv(link_f,sep=',')\n",
    "df = pd.merge(df,df_id,on=['movieId'])\n",
    "\n",
    "rating_matrix = np.zeros((df.userId.unique().shape[0],max(df.movieId)))\n",
    "for row in df.itertuples():\n",
    "    rating_matrix[row[1]-1,row[2]-1] = row[3]\n",
    "rating_matrix = rating_matrix[:,:9000]\n",
    "\n",
    "#Evaluate sparsity of matrix\n",
    "sparsity = float(len(rating_matrix.nonzero()[0]))\n",
    "sparsity /= (rating_matrix.shape[0]*rating_matrix.shape[1])\n",
    "sparsity *= 100\n",
    "print('Sparsity is {0}%'.format(sparsity))\n",
    "\n",
    "#Splite to train/test matrix\n",
    "train_matrix = rating_matrix.copy()\n",
    "test_matrix = np.zeros(rating_matrix.shape)\n",
    "\n",
    "for i in range(rating_matrix.shape[0]):\n",
    "    rating_index = np.random.choice(rating_matrix[i,:].nonzero()[0],size=10,replace=True)\n",
    "    train_matrix[i,rating_index] = 0.0\n",
    "    test_matrix[i,rating_index] = rating_matrix[i,rating_index]\n",
    "\n",
    "#Cosine similarity\n",
    "similarity_usr = train_matrix.dot(train_matrix.T) + 1e-9\n",
    "norms = np.array([np.sqrt(np.diagonal(similarity_usr))])\n",
    "similarity_usr = (similarity_usr/(norms*norms.T))\n",
    "\n",
    "similarity_mv = train_matrix.T.dot(train_matrix) + 1e-9\n",
    "norms = np.array([np.sqrt(np.diagonal(similarity_mv))])\n",
    "similarity_mv = (similarity_mv/(norms*norms.T))\n",
    "\n",
    "prediction = similarity_usr.dot(train_matrix)/np.array([np.abs(similarity_usr).sum(axis=1)]).T\n",
    "prediction = prediction[test_matrix.nonzero()]\n",
    "test_vector = test_matrix[test_matrix.nonzero()]\n",
    "mse = mean_squared_error(prediction,test_vector)\n",
    "\n",
    "print('mse: {0}'.format(mse))\n",
    "\n",
    "#Test\n",
    "import requests\n",
    "import json\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "\n",
    "k = 10\n",
    "n_display = 5\n",
    "base_mv_idx = 0\n",
    "idx_to_mv = {}\n",
    "\n",
    "for row in df_id.itertuples():\n",
    "    idx_to_mv[row[1]-1] = row[2]\n",
    "mv = [idx_to_mv[x] for x in np.argsort(similarity_mv[base_mv_idx])[:-k-1:-1]]\n",
    "mv = filter(lambda imdb: len(str(imdb))==6, mv)\n",
    "mv = list(mv)[:n_display]\n",
    "\n",
    "#Get posters from Movie Database by API\n",
    "headers = {'Accept':'application/json'}\n",
    "payload = {'api_key':'20047cd838219fb54d1f8fc32c45cda4'}\n",
    "response = requests.get('http://api.themoviedb.org/3/configuration',params=payload,headers=headers)\n",
    "response = json.loads(response.text)\n",
    "\n",
    "base_url = response['images']['base_url']+'w185'\n",
    "\n",
    "def get_poster(imdb,base_url):\n",
    "    #query themovie.org API for movie poster path.\n",
    "    imdb_id = 'tt0{0}'.format(imdb)\n",
    "    movie_url = 'http://api.themoviedb.org/3/movie/{:}/images'.format(imdb_id)\n",
    "    response = requests.get(movie_url, params=payload, headers=headers)\n",
    "\n",
    "    try:\n",
    "        file_path = json.loads(response.text)['posters'][0]['file_path']\n",
    "    except:\n",
    "        print('Something wrong, cannot get the poster for imdb id: {0}!'.format(imdb))\n",
    "\n",
    "    return base_url+file_path\n",
    "\n",
    "\n",
    "URL = [0]*len(mv)\n",
    "for i,m in enumerate(mv):\n",
    "    URL[i] = get_poster(m,base_url)\n",
    "\n",
    "images = ''\n",
    "for i in range(len(mv)):\n",
    "    images+=\"<img style='width: 100px; margin: 0px; float: left; border: 1px solid black;' src='%s' />\" % URL[i]\n",
    "\n",
    "#print(images)\n",
    "display(HTML(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
